# Millisecond AI: LLM Inference at the 5G Edge

## Global Data & AI Virtual Tech Conference 2025

This repository supports the "Millisecond AI" session from the Global Data & AI Virtual Tech Conference 2025. It demonstrates how to deploy a production-grade AI inference pipeline at the 5G edge using open-source technologies and CPU-based infrastructure.

---

## Table of Contents

1. [Overall Solution](#overall-solution)
2. [System Architecture](#system-architecture)
3. [Inference Server Configuration at 5G Edge](#inference-server-configuration-at-5g-edge)
   - [EC2 Inference Server Configuration](#ec2-inference-server-configuration)
     - [Phase 1: EC2 Instance Setup](#phase-1-ec2-instance-setup)
     - [Phase 2: System Dependencies Installation](#phase-2-system-dependencies-installation)
     - [Phase 3: Building llama.cpp from Source](#phase-3-building-llamacpp-from-source)
     - [Phase 4: Downloading the LLM Model](#phase-4-downloading-the-llm-model)
     - [Phase 5: Running Inference](#phase-5-running-inference)
4. [Results](#results)
5. [Conclusions & Takeaways](#conclusions--takeaways)

---
